{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, classification_report,mean_squared_error, precision_score, recall_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from statsmodels.discrete.discrete_model import MNLogit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the dataset which comes from short_dataset_feature_engineering.ipynb \n",
    "file_path = '/data/caysar9/results/final_short.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency counts before SMOTE:\n",
      "Frequency counts train set\n",
      "affected_activity_QoL\n",
      "0    7038\n",
      "1    4483\n",
      "Name: count, dtype: int64\n",
      "Frequency counts test set\n",
      "affected_activity_QoL\n",
      "0    1760\n",
      "1    1121\n",
      "Name: count, dtype: int64\n",
      "Frequency counts after SMOTE:\n",
      "Frequency counts train set\n",
      "affected_activity_QoL\n",
      "1    7038\n",
      "0    7038\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define features and target for affected activities QoL\n",
    "features = ['migraine_days_per_month', 'painintensity', 'duration_in_hours', 'trigger_stress', 'trigger_poor_sleep','sleep_duration_hours','sleep_duration_past_7_days','migraine_attacks_past7days','mean_migraine_duration_past7days','reported_anxiety', 'reported_depression']\n",
    "\n",
    "X = df[features]\n",
    "y = df['affected_activity_QoL']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Frequency counts before applying SMOTE\n",
    "print(\"Frequency counts before SMOTE:\")\n",
    "print(\"Frequency counts train set\")\n",
    "print(y_train.value_counts())\n",
    "print(\"Frequency counts test set\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "# Frequency counts before applying SMOTE\n",
    "print(\"Frequency counts after SMOTE:\")\n",
    "print(\"Frequency counts train set\")\n",
    "print(y_train_balanced.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "36 fits failed out of a total of 90.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 75, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "27 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.65649336 0.65657869\n",
      " 0.6565409  0.65649881 0.65649845        nan 0.65654217        nan\n",
      " 0.6565005  0.65652933        nan 0.65652098 0.65649869 0.65652176\n",
      " 0.65650044        nan        nan 0.65669316 0.65651268 0.6565515\n",
      " 0.6565578  0.65650904        nan        nan        nan 0.65659402]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 0.46227288910538067, 'max_iter': 395, 'penalty': 'l1', 'solver': 'saga'}\n",
      "\n",
      "AUC Score: 0.7052\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1149  611]\n",
      " [ 397  724]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.65      0.70      1760\n",
      "           1       0.54      0.65      0.59      1121\n",
      "\n",
      "    accuracy                           0.65      2881\n",
      "   macro avg       0.64      0.65      0.64      2881\n",
      "weighted avg       0.67      0.65      0.65      2881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],  # Regularization methods\n",
    "    'C': uniform(0.01, 10),  # Continuous uniform distribution for regularization strength\n",
    "    'solver': ['liblinear', 'saga'],  # solvers\n",
    "    'max_iter': randint(100, 1000)  # Randomly sample number of iterations\n",
    "}\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lr_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Evaluate based on AUC\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,  # Show progress\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Train the best model on the balanced training set\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate and display AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.648446\n",
      "         Iterations 5\n",
      "                             Logit Regression Results                            \n",
      "=================================================================================\n",
      "Dep. Variable:     affected_activity_QoL   No. Observations:                14076\n",
      "Model:                             Logit   Df Residuals:                    14064\n",
      "Method:                              MLE   Df Model:                           11\n",
      "Date:                   Sat, 07 Dec 2024   Pseudo R-squ.:                 0.06449\n",
      "Time:                           17:28:21   Log-Likelihood:                -9127.5\n",
      "converged:                          True   LL-Null:                       -9756.7\n",
      "Covariance Type:               nonrobust   LLR p-value:                4.115e-263\n",
      "====================================================================================================\n",
      "                                       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "const                               -1.8552      0.107    -17.412      0.000      -2.064      -1.646\n",
      "migraine_days_per_month              0.0219      0.010      2.110      0.035       0.002       0.042\n",
      "painintensity                        0.1992      0.010     20.563      0.000       0.180       0.218\n",
      "duration_in_hours                    0.0275      0.008      3.443      0.001       0.012       0.043\n",
      "trigger_stress                       0.0417      0.045      0.928      0.353      -0.046       0.130\n",
      "trigger_poor_sleep                   0.7171      0.044     16.298      0.000       0.631       0.803\n",
      "sleep_duration_hours                -0.0224      0.019     -1.212      0.226      -0.059       0.014\n",
      "sleep_duration_past_7_days           0.0192      0.021      0.918      0.359      -0.022       0.060\n",
      "migraine_attacks_past7days           0.0033      0.017      0.194      0.846      -0.030       0.036\n",
      "mean_migraine_duration_past7days     0.0061      0.009      0.668      0.504      -0.012       0.024\n",
      "reported_anxiety                     0.6468      0.054     12.035      0.000       0.541       0.752\n",
      "reported_depression                  0.0903      0.053      1.693      0.090      -0.014       0.195\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "X_train_balanced_const = sm.add_constant(X_train_balanced)\n",
    "\n",
    "# Fit the logistic regression model on the balanced data\n",
    "logit_model = sm.Logit(y_train_balanced, X_train_balanced_const).fit()\n",
    "\n",
    "# Display the summary of the model\n",
    "print(logit_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best Hyperparameters: {'criterion': 'log_loss', 'max_depth': 7, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 8}\n",
      "\n",
      "AUC Score: 0.6895\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1129  631]\n",
      " [ 401  720]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.64      0.69      1760\n",
      "           1       0.53      0.64      0.58      1121\n",
      "\n",
      "    accuracy                           0.64      2881\n",
      "   macro avg       0.64      0.64      0.63      2881\n",
      "weighted avg       0.66      0.64      0.65      2881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # Splitting criteria\n",
    "    'max_depth': randint(5, 50),  # Randomly sample tree depth between 5 and 50\n",
    "    'min_samples_split': randint(2, 20),  # Minimum samples required to split a node\n",
    "    'min_samples_leaf': randint(1, 10),  # Minimum samples required in a leaf node\n",
    "    'max_features': [None, 'sqrt', 'log2']  # Number of features to consider for best split\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=dt_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Evaluate based on AUC\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,  # Show progress\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Train the best model on the balanced training set\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate and display AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': 30, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 267}\n",
      "\n",
      "AUC Score: 0.6610\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1241  519]\n",
      " [ 559  562]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.71      0.70      1760\n",
      "           1       0.52      0.50      0.51      1121\n",
      "\n",
      "    accuracy                           0.63      2881\n",
      "   macro avg       0.60      0.60      0.60      2881\n",
      "weighted avg       0.62      0.63      0.62      2881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 300),  # Randomly sample between 50 and 300 trees\n",
    "    'max_depth': [None, 10, 20, 30],  # Fixed choices for tree depth\n",
    "    'min_samples_split': randint(2, 11),  # Randomly sample between 2 and 10 for splitting\n",
    "    'min_samples_leaf': randint(1, 5),  # Randomly sample between 1 and 4 for leaf nodes\n",
    "    'max_features': ['sqrt', 'log2', None],  # Fixed options for max features\n",
    "    'bootstrap': [True, False]  # Fixed options for bootstrapping\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Evaluate based on AUC\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,  # Show progress\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Train the best model on the balanced training set\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate and display AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importances:\n",
      "\n",
      "                             Feature  Importance\n",
      "6         sleep_duration_past_7_days    0.167812\n",
      "2                  duration_in_hours    0.164579\n",
      "8   mean_migraine_duration_past7days    0.163901\n",
      "5               sleep_duration_hours    0.163865\n",
      "1                      painintensity    0.133761\n",
      "0            migraine_days_per_month    0.072566\n",
      "7         migraine_attacks_past7days    0.049810\n",
      "4                 trigger_poor_sleep    0.028270\n",
      "9                   reported_anxiety    0.024321\n",
      "3                     trigger_stress    0.016409\n",
      "10               reported_depression    0.014706\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "feature_importances = best_model.feature_importances_\n",
    "feature_names = X_train_balanced.columns\n",
    "\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\\n\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.17838315927084888, 'max_depth': 9, 'max_features': 'log2', 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 186, 'subsample': 0.9131988669057362}\n",
      "\n",
      "AUC Score: 0.6552\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1252  508]\n",
      " [ 551  570]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.71      0.70      1760\n",
      "           1       0.53      0.51      0.52      1121\n",
      "\n",
      "    accuracy                           0.63      2881\n",
      "   macro avg       0.61      0.61      0.61      2881\n",
      "weighted avg       0.63      0.63      0.63      2881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform,randint\n",
    "\n",
    "\n",
    "# Define the hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 200),  # Number of boosting stages\n",
    "    'learning_rate': uniform(0.01, 0.3),  # Learning rate for boosting\n",
    "    'max_depth': randint(3, 10),  # Maximum depth of individual estimators\n",
    "    'min_samples_split': randint(2, 10),  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': randint(1, 5),  # Minimum number of samples required to be a leaf node\n",
    "    'subsample': uniform(0.7, 0.3),  # Fraction of samples used for fitting each base learner\n",
    "    'max_features': ['sqrt', 'log2', None]  # Number of features to consider at each split\n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gb_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Evaluate based on AUC\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,  # Show progress\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Train the best model on the balanced training set\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate and display AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importances:\n",
      "\n",
      "                             Feature  Importance\n",
      "1                      painintensity    0.460308\n",
      "4                 trigger_poor_sleep    0.182021\n",
      "9                   reported_anxiety    0.106767\n",
      "8   mean_migraine_duration_past7days    0.078362\n",
      "5               sleep_duration_hours    0.047898\n",
      "6         sleep_duration_past_7_days    0.032237\n",
      "2                  duration_in_hours    0.029339\n",
      "0            migraine_days_per_month    0.025729\n",
      "3                     trigger_stress    0.014883\n",
      "10               reported_depression    0.014712\n",
      "7         migraine_attacks_past7days    0.007744\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "feature_importances = best_model.feature_importances_\n",
    "feature_names = X_train_balanced.columns\n",
    "\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\\n\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best Hyperparameters: {'metric': 'manhattan', 'n_neighbors': 4, 'p': 1, 'weights': 'distance'}\n",
      "\n",
      "AUC Score: 0.5931\n",
      "Confusion Matrix:\n",
      "[[1109  651]\n",
      " [ 567  554]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.63      0.65      1760\n",
      "           1       0.46      0.49      0.48      1121\n",
      "\n",
      "    accuracy                           0.58      2881\n",
      "   macro avg       0.56      0.56      0.56      2881\n",
      "weighted avg       0.58      0.58      0.58      2881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'n_neighbors': randint(1, 50),  # Number of neighbors to test\n",
    "    'weights': ['uniform', 'distance'],  # Weighting strategies\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],  # Distance metrics\n",
    "    'p': randint(1, 3)  # distance parameter \n",
    "}\n",
    "\n",
    "# Initialize the K-Nearest Neighbors model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=knn_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Evaluate based on AUC\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,  # Show progress\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Train the best model on the balanced training set\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate and display AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# kernel dies when jupyter notebook tries import tensor flow so I tried this code in my local machine and commented it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Define features and target\n",
    "features = ['migraine_days_per_month', 'painintensity', 'duration_in_hours', 'trigger_stress', 'trigger_poor_sleep',\n",
    "            'sleep_duration_hours', 'sleep_duration_past_7_days', 'migraine_attacks_past7days',\n",
    "            'mean_migraine_duration_past7days', 'reported_anxiety', 'reported_depression']\n",
    "\n",
    "X = df[features]\n",
    "y = df['affected_activity_QoL']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE to the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define a function to create the model\n",
    "def create_model(learning_rate=0.001, neurons=[64, 32, 16]):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons[0], activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "    model.add(Dense(neurons[1], activation='relu'))\n",
    "    model.add(Dense(neurons[2], activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model for compatibility with RandomizedSearchCV\n",
    "keras_clf = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# Define hyperparameter distributions\n",
    "param_distributions = {\n",
    "    \"model__learning_rate\": [0.001, 0.01, 0.1],\n",
    "    \"model__neurons\": [[64, 32, 16], [128, 64, 32], [32, 16, 8]],\n",
    "    \"batch_size\": [16, 32, 64],\n",
    "    \"epochs\": [20, 50, 100]\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=keras_clf,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=10,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Focus on auc for hyperparameter tuning\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "random_search.fit(X_train_scaled, y_train_smote)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]  # Probabilities for the positive class\n",
    "y_pred_classes = (y_pred_proba > 0.5).astype(int)  # Convert probabilities to binary classes\n",
    "\n",
    "# Calculate AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_classes))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "RESULTS\n",
    "Best Hyperparameters: {'model__neurons': [64, 32, 16], 'model__learning_rate': 0.01, 'epochs': 20, 'batch_size': 32}\n",
    "\n",
    "AUC Score: 0.6932\n",
    "Confusion Matrix:\n",
    "[[1194  566]\n",
    " [ 449  672]]\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.73      0.68      0.70      1760\n",
    "           1       0.54      0.60      0.57      1121\n",
    "\n",
    "    accuracy                           0.65      2881\n",
    "   macro avg       0.63      0.64      0.64      2881\n",
    "weighted avg       0.66      0.65      0.65      2881"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
