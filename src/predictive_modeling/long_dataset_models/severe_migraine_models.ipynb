{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, classification_report,mean_squared_error, precision_score, recall_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from statsmodels.discrete.discrete_model import MNLogit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the dataset which comes from long_dataset_feature_engineering.ipynb \n",
    "file_path = '/data/caysar9/results/final_long.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency counts before SMOTE:\n",
      "Frequency counts train set\n",
      "severe_migraine\n",
      "0    40897\n",
      "1     4731\n",
      "Name: count, dtype: int64\n",
      "Frequency counts test set\n",
      "severe_migraine\n",
      "0    10224\n",
      "1     1183\n",
      "Name: count, dtype: int64\n",
      "Frequency counts after SMOTE:\n",
      "Frequency counts train set\n",
      "severe_migraine\n",
      "0    40897\n",
      "1    40897\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define features and target\n",
    "features = ['trigger_lack_physical_activity','trigger_physical_activity','trigger_poor_sleep','trigger_stress','sleep_duration_hours','sleep_duration_past_7_days','age','gender_encoded','migraine_attacks_past7days','mean_migraine_duration_past7days']\n",
    "\n",
    "\n",
    "X = df[features]\n",
    "y = df['severe_migraine']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Frequency counts before applying SMOTE\n",
    "print(\"Frequency counts before SMOTE:\")\n",
    "print(\"Frequency counts train set\")\n",
    "print(y_train.value_counts())\n",
    "print(\"Frequency counts test set\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "# Frequency counts before applying SMOTE\n",
    "print(\"Frequency counts after SMOTE:\")\n",
    "print(\"Frequency counts train set\")\n",
    "print(y_train_balanced.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "36 fits failed out of a total of 90.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 75, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "27 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.85585335 0.85620373\n",
      " 0.85582515 0.85582964 0.8558317         nan 0.85582518        nan\n",
      " 0.85582807 0.85582912        nan 0.85583109 0.85583245 0.85583331\n",
      " 0.85582804        nan        nan 0.85584651 0.85582756 0.85582583\n",
      " 0.85582618 0.85582727        nan        nan        nan 0.85583493]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 0.017787658410143285, 'max_iter': 376, 'penalty': 'l1', 'solver': 'saga'}\n",
      "\n",
      "AUC Score: 0.8022\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8235 1989]\n",
      " [ 410  773]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.81      0.87     10224\n",
      "           1       0.28      0.65      0.39      1183\n",
      "\n",
      "    accuracy                           0.79     11407\n",
      "   macro avg       0.62      0.73      0.63     11407\n",
      "weighted avg       0.88      0.79      0.82     11407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],  # Regularization methods\n",
    "    'C': uniform(0.01, 10),  # Continuous uniform distribution for regularization strength\n",
    "    'solver': ['liblinear', 'saga'],  # solvers\n",
    "    'max_iter': randint(100, 1000)  # Randomly sample number of iterations\n",
    "}\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lr_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Evaluate based on AUC\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,  # Show progress\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Train the best model on the balanced training set\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate and display AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.482364\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:        severe_migraine   No. Observations:                81794\n",
      "Model:                          Logit   Df Residuals:                    81783\n",
      "Method:                           MLE   Df Model:                           10\n",
      "Date:                Sat, 07 Dec 2024   Pseudo R-squ.:                  0.3041\n",
      "Time:                        20:04:05   Log-Likelihood:                -39455.\n",
      "converged:                       True   LL-Null:                       -56695.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "====================================================================================================\n",
      "                                       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "const                               -1.5145      0.072    -21.025      0.000      -1.656      -1.373\n",
      "trigger_lack_physical_activity      -1.1780      0.044    -26.718      0.000      -1.264      -1.092\n",
      "trigger_physical_activity           -1.1496      0.060    -19.164      0.000      -1.267      -1.032\n",
      "trigger_poor_sleep                  -0.7481      0.021    -34.924      0.000      -0.790      -0.706\n",
      "trigger_stress                      -0.5924      0.020    -29.113      0.000      -0.632      -0.552\n",
      "sleep_duration_hours                -0.0469      0.006     -8.514      0.000      -0.058      -0.036\n",
      "sleep_duration_past_7_days          -0.0259      0.008     -3.221      0.001      -0.042      -0.010\n",
      "age                                 -0.0203      0.001    -24.520      0.000      -0.022      -0.019\n",
      "gender_encoded                      -0.3426      0.023    -14.780      0.000      -0.388      -0.297\n",
      "migraine_attacks_past7days           0.1048      0.005     21.130      0.000       0.095       0.115\n",
      "mean_migraine_duration_past7days     0.1399      0.001    129.688      0.000       0.138       0.142\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "X_train_balanced_const = sm.add_constant(X_train_balanced)\n",
    "\n",
    "# Fit the logistic regression model on the balanced data\n",
    "logit_model = sm.Logit(y_train_balanced, X_train_balanced_const).fit()\n",
    "\n",
    "# Display the summary of the model\n",
    "print(logit_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best Hyperparameters: {'criterion': 'entropy', 'max_depth': 26, 'max_features': None, 'min_samples_leaf': 9, 'min_samples_split': 18}\n",
      "\n",
      "AUC Score: 0.7216\n",
      "\n",
      "Confusion Matrix:\n",
      "[[8452 1772]\n",
      " [ 671  512]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.87     10224\n",
      "           1       0.22      0.43      0.30      1183\n",
      "\n",
      "    accuracy                           0.79     11407\n",
      "   macro avg       0.58      0.63      0.58     11407\n",
      "weighted avg       0.85      0.79      0.81     11407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # Splitting criteria\n",
    "    'max_depth': randint(5, 50),  # Randomly sample tree depth between 5 and 50\n",
    "    'min_samples_split': randint(2, 20),  # Minimum samples required to split a node\n",
    "    'min_samples_leaf': randint(1, 10),  # Minimum samples required in a leaf node\n",
    "    'max_features': [None, 'sqrt', 'log2']  # Number of features to consider for best split\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=dt_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Evaluate based on AUC\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,  # Show progress\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Train the best model on the balanced training set\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate and display AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': 30, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 267}\n",
      "\n",
      "AUC Score: 0.7934\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9106 1118]\n",
      " [ 762  421]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91     10224\n",
      "           1       0.27      0.36      0.31      1183\n",
      "\n",
      "    accuracy                           0.84     11407\n",
      "   macro avg       0.60      0.62      0.61     11407\n",
      "weighted avg       0.86      0.84      0.84     11407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 300),  # Randomly sample between 50 and 300 trees\n",
    "    'max_depth': [None, 10, 20, 30],  # Fixed choices for tree depth\n",
    "    'min_samples_split': randint(2, 11),  # Randomly sample between 2 and 10 for splitting\n",
    "    'min_samples_leaf': randint(1, 5),  # Randomly sample between 1 and 4 for leaf nodes\n",
    "    'max_features': ['sqrt', 'log2', None],  # Fixed options for max features\n",
    "    'bootstrap': [True, False]  # Fixed options for bootstrapping\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Evaluate based on AUC\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,  # Show progress\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Train the best model on the balanced training set\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate and display AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importances:\n",
      "\n",
      "                            Feature  Importance\n",
      "9  mean_migraine_duration_past7days    0.481145\n",
      "6                               age    0.142572\n",
      "4              sleep_duration_hours    0.121943\n",
      "5        sleep_duration_past_7_days    0.118380\n",
      "8        migraine_attacks_past7days    0.059767\n",
      "2                trigger_poor_sleep    0.023258\n",
      "0    trigger_lack_physical_activity    0.015921\n",
      "3                    trigger_stress    0.015567\n",
      "7                    gender_encoded    0.014542\n",
      "1         trigger_physical_activity    0.006905\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances from the trained Random Forest model\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train_balanced.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\\n\")\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.268219174976903, 'max_depth': 9, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 4, 'n_estimators': 186, 'subsample': 0.8454489914076949}\n",
      "\n",
      "AUC Score: 0.7921\n",
      "\n",
      "Confusion Matrix:\n",
      "[[9201 1023]\n",
      " [ 741  442]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91     10224\n",
      "           1       0.30      0.37      0.33      1183\n",
      "\n",
      "    accuracy                           0.85     11407\n",
      "   macro avg       0.61      0.64      0.62     11407\n",
      "weighted avg       0.86      0.85      0.85     11407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 200),  # Number of boosting stages\n",
    "    'learning_rate': uniform(0.01, 0.3),  # Learning rate for boosting\n",
    "    'max_depth': randint(3, 10),  # Maximum depth of individual estimators\n",
    "    'min_samples_split': randint(2, 10),  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': randint(1, 5),  # Minimum number of samples required to be a leaf node\n",
    "    'subsample': uniform(0.7, 0.3),  # Fraction of samples used for fitting each base learner\n",
    "    'max_features': ['sqrt', 'log2', None]  # Number of features to consider at each split\n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gb_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Evaluate based on AUC\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,  # Show progress\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Train the best model on the balanced training set\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate and display AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importances:\n",
      "\n",
      "                            Feature  Importance\n",
      "9  mean_migraine_duration_past7days    0.522913\n",
      "6                               age    0.147251\n",
      "8        migraine_attacks_past7days    0.085479\n",
      "4              sleep_duration_hours    0.076219\n",
      "5        sleep_duration_past_7_days    0.074679\n",
      "2                trigger_poor_sleep    0.032856\n",
      "0    trigger_lack_physical_activity    0.019874\n",
      "3                    trigger_stress    0.018092\n",
      "7                    gender_encoded    0.015499\n",
      "1         trigger_physical_activity    0.007139\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances from the trained Gradient boosting model\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display the feature importances alongside feature names\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train_balanced.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display the feature importance table\n",
    "print(\"\\nFeature Importances:\\n\")\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'metric': 'manhattan', 'n_neighbors': 6, 'p': 2, 'weights': 'distance'}\n",
      "\n",
      "AUC Score: 0.7579\n",
      "Confusion Matrix:\n",
      "[[8198 2026]\n",
      " [ 528  655]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.80      0.87     10224\n",
      "           1       0.24      0.55      0.34      1183\n",
      "\n",
      "    accuracy                           0.78     11407\n",
      "   macro avg       0.59      0.68      0.60     11407\n",
      "weighted avg       0.87      0.78      0.81     11407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'n_neighbors': randint(1, 50),  # Number of neighbors to test\n",
    "    'weights': ['uniform', 'distance'],  # Weighting strategies\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],  # Distance metrics\n",
    "    'p': randint(1, 3)  # distance parameter \n",
    "}\n",
    "\n",
    "# Initialize the K-Nearest Neighbors model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=knn_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Evaluate based on AUC\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,  # Show progress\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Train the best model on the balanced training set\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate and display AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\nfrom tensorflow.keras.optimizers import Adam\\nfrom sklearn.model_selection import RandomizedSearchCV\\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.model_selection import train_test_split\\nfrom imblearn.over_sampling import SMOTE\\nfrom scikeras.wrappers import KerasClassifier\\nimport numpy as np\\n\\n# Define features and target\\nfeatures = [\\'trigger_lack_physical_activity\\',\\'trigger_physical_activity\\',\\'trigger_poor_sleep\\',\\'trigger_stress\\',\\'sleep_duration_hours\\',\\'sleep_duration_past_7_days\\',\\'age\\',\\'gender_encoded\\',\\'migraine_attacks_past7days\\',\\'mean_migraine_duration_past7days\\']\\n\\nX = df[features]\\ny = df[\\'affected_activity_QoL\\']\\n\\n# Split data into training and test sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\\n\\n# Apply SMOTE to the training set\\nsmote = SMOTE(random_state=42)\\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\\n\\n# Scale the features\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train_smote)\\nX_test_scaled = scaler.transform(X_test)\\n\\n# Define a function to create the model\\ndef create_model(learning_rate=0.001, neurons=[64, 32, 16]):\\n    model = Sequential()\\n    model.add(Dense(neurons[0], activation=\\'relu\\', input_shape=(X_train_scaled.shape[1],)))\\n    model.add(Dense(neurons[1], activation=\\'relu\\'))\\n    model.add(Dense(neurons[2], activation=\\'relu\\'))\\n    model.add(Dense(1, activation=\\'sigmoid\\'))  # Binary classification\\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=\\'binary_crossentropy\\', metrics=[\\'accuracy\\'])\\n    return model\\n\\n# Wrap the model for compatibility with RandomizedSearchCV\\nkeras_clf = KerasClassifier(model=create_model, verbose=0)\\n\\n# Define hyperparameter distributions\\nparam_distributions = {\\n    \"model__learning_rate\": [0.001, 0.01, 0.1],\\n    \"model__neurons\": [[64, 32, 16], [128, 64, 32], [32, 16, 8]],\\n    \"batch_size\": [16, 32, 64],\\n    \"epochs\": [20, 50, 100]\\n}\\n\\n# Set up RandomizedSearchCV\\nrandom_search = RandomizedSearchCV(\\n    estimator=keras_clf,\\n    param_distributions=param_distributions,\\n    n_iter=10,  # Number of random combinations to try\\n    scoring=\\'roc_auc\\',  # Focus on auc for hyperparameter tuning\\n    cv=3,  # 3-fold cross-validation\\n    verbose=1,\\n    n_jobs=-1,  # Use all available cores\\n    random_state=42\\n)\\n\\n# Perform hyperparameter tuning\\nrandom_search.fit(X_train_scaled, y_train_smote)\\n\\n# Get the best model and parameters\\nbest_model = random_search.best_estimator_\\nprint(\"Best Hyperparameters:\", random_search.best_params_)\\n\\n# Evaluate the model on the test set\\ny_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]  # Probabilities for the positive class\\ny_pred_classes = (y_pred_proba > 0.5).astype(int)  # Convert probabilities to binary classes\\n\\n# Calculate AUC\\nauc_score = roc_auc_score(y_test, y_pred_proba)\\nprint(f\"\\nAUC Score: {auc_score:.4f}\")\\n\\n# Confusion Matrix and Classification Report\\nprint(\"Confusion Matrix:\")\\nprint(confusion_matrix(y_test, y_pred_classes))\\nprint(\"\\nClassification Report:\")\\nprint(classification_report(y_test, y_pred_classes))\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Define features and target\n",
    "features = ['trigger_lack_physical_activity','trigger_physical_activity','trigger_poor_sleep','trigger_stress','sleep_duration_hours','sleep_duration_past_7_days','age','gender_encoded','migraine_attacks_past7days','mean_migraine_duration_past7days']\n",
    "\n",
    "X = df[features]\n",
    "y = df['severe_migraine']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE to the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define a function to create the model\n",
    "def create_model(learning_rate=0.001, neurons=[64, 32, 16]):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons[0], activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "    model.add(Dense(neurons[1], activation='relu'))\n",
    "    model.add(Dense(neurons[2], activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model for compatibility with RandomizedSearchCV\n",
    "keras_clf = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# Define hyperparameter distributions\n",
    "param_distributions = {\n",
    "    \"model__learning_rate\": [0.001, 0.01, 0.1],\n",
    "    \"model__neurons\": [[64, 32, 16], [128, 64, 32], [32, 16, 8]],\n",
    "    \"batch_size\": [16, 32, 64],\n",
    "    \"epochs\": [20, 50, 100]\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=keras_clf,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=10,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Focus on auc for hyperparameter tuning\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "random_search.fit(X_train_scaled, y_train_smote)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]  # Probabilities for the positive class\n",
    "y_pred_classes = (y_pred_proba > 0.5).astype(int)  # Convert probabilities to binary classes\n",
    "\n",
    "# Calculate AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_classes))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "RESULTS:\n",
    "\n",
    "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
    "\n",
    "Best Hyperparameters: {'model__neurons': [128, 64, 32], 'model__learning_rate': 0.001, 'epochs': 50, 'batch_size': 16}\n",
    "\n",
    "AUC Score: 0.7771\n",
    "Confusion Matrix:\n",
    "[[7848 2376]\n",
    " [ 470  713]]\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.94      0.77      0.85     10224\n",
    "           1       0.23      0.60      0.33      1183\n",
    "\n",
    "    accuracy                           0.75     11407\n",
    "   macro avg       0.59      0.69      0.59     11407\n",
    "weighted avg       0.87      0.75      0.79     11407"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
