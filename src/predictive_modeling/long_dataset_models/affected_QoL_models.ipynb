{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, classification_report,mean_squared_error, precision_score, recall_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from statsmodels.discrete.discrete_model import MNLogit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the dataset which comes from long_dataset_feature_engineering.ipynb \n",
    "file_path = '/data/caysar9/results/final_long.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency counts before SMOTE:\n",
      "Frequency counts train set\n",
      "affected_activity_QoL\n",
      "1    24757\n",
      "0    20871\n",
      "Name: count, dtype: int64\n",
      "Frequency counts test set\n",
      "affected_activity_QoL\n",
      "1    6189\n",
      "0    5218\n",
      "Name: count, dtype: int64\n",
      "Frequency counts after SMOTE:\n",
      "Frequency counts train set\n",
      "affected_activity_QoL\n",
      "1    24757\n",
      "0    24757\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define features and target for affected activities QoL\n",
    "features = ['migraine_days_per_month','attack_duration_hours','pain_intensity', 'trigger_stress', 'trigger_poor_sleep','sleep_duration_hours','sleep_duration_past_7_days','migraine_attacks_past7days','mean_migraine_duration_past7days']\n",
    "\n",
    "\n",
    "X = df[features]\n",
    "y = df['affected_activity_QoL']\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Frequency counts before applying SMOTE\n",
    "print(\"Frequency counts before SMOTE:\")\n",
    "print(\"Frequency counts train set\")\n",
    "print(y_train.value_counts())\n",
    "print(\"Frequency counts test set\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "# Frequency counts after applying SMOTE\n",
    "print(\"Frequency counts after SMOTE:\")\n",
    "print(\"Frequency counts train set\")\n",
    "print(y_train_balanced.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "36 fits failed out of a total of 90.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 75, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "27 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/data/caysar9/venv-py311/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.66945039 0.66924987\n",
      " 0.66945344 0.66944888 0.66944869        nan 0.66945339        nan\n",
      " 0.66944879 0.6694481         nan 0.66945442 0.66944885 0.66945528\n",
      " 0.66944878        nan        nan 0.66944303 0.66944822 0.66945376\n",
      " 0.66945418 0.66944836        nan        nan        nan 0.66944663]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 5.210680211778108, 'max_iter': 205, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "\n",
      "AUC Score: 0.6686\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3230 1988]\n",
      " [2328 3861]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.62      0.60      5218\n",
      "           1       0.66      0.62      0.64      6189\n",
      "\n",
      "    accuracy                           0.62     11407\n",
      "   macro avg       0.62      0.62      0.62     11407\n",
      "weighted avg       0.62      0.62      0.62     11407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],  # Regularization methods\n",
    "    'C': uniform(0.01, 10),  # Continuous uniform distribution for regularization strength\n",
    "    'solver': ['liblinear', 'saga'],  # solvers\n",
    "    'max_iter': randint(100, 1000)  # Randomly sample number of iterations\n",
    "}\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lr_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Evaluate based on AUC\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,  # Show progress\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Train the best model on the balanced training set\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate and display AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.646631\n",
      "         Iterations 5\n",
      "                             Logit Regression Results                            \n",
      "=================================================================================\n",
      "Dep. Variable:     affected_activity_QoL   No. Observations:                49514\n",
      "Model:                             Logit   Df Residuals:                    49504\n",
      "Method:                              MLE   Df Model:                            9\n",
      "Date:                   Sat, 07 Dec 2024   Pseudo R-squ.:                 0.06711\n",
      "Time:                           17:00:04   Log-Likelihood:                -32017.\n",
      "converged:                          True   LL-Null:                       -34320.\n",
      "Covariance Type:               nonrobust   LLR p-value:                     0.000\n",
      "====================================================================================================\n",
      "                                       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "const                               -2.1152      0.066    -31.975      0.000      -2.245      -1.986\n",
      "migraine_days_per_month              0.0526      0.005     10.433      0.000       0.043       0.062\n",
      "attack_duration_hours                0.0112      0.001     11.360      0.000       0.009       0.013\n",
      "pain_intensity                       0.2675      0.005     50.434      0.000       0.257       0.278\n",
      "trigger_stress                       0.4073      0.020     20.043      0.000       0.367       0.447\n",
      "trigger_poor_sleep                   0.2873      0.021     13.757      0.000       0.246       0.328\n",
      "sleep_duration_hours                 0.0158      0.006      2.698      0.007       0.004       0.027\n",
      "sleep_duration_past_7_days          -0.0180      0.008     -2.154      0.031      -0.034      -0.002\n",
      "migraine_attacks_past7days          -0.0182      0.005     -3.543      0.000      -0.028      -0.008\n",
      "mean_migraine_duration_past7days     0.0062      0.001      4.498      0.000       0.003       0.009\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "X_train_balanced_const = sm.add_constant(X_train_balanced)\n",
    "\n",
    "# Fit the logistic regression model on the balanced data\n",
    "logit_model = sm.Logit(y_train_balanced, X_train_balanced_const).fit()\n",
    "\n",
    "# Display the summary of the model\n",
    "print(logit_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best Hyperparameters: {'criterion': 'log_loss', 'max_depth': 7, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 8}\n",
      "\n",
      "AUC Score: 0.6678\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3110 2108]\n",
      " [2228 3961]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.60      0.59      5218\n",
      "           1       0.65      0.64      0.65      6189\n",
      "\n",
      "    accuracy                           0.62     11407\n",
      "   macro avg       0.62      0.62      0.62     11407\n",
      "weighted avg       0.62      0.62      0.62     11407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # Splitting criteria\n",
    "    'max_depth': randint(5, 50),  # Randomly sample tree depth between 5 and 50\n",
    "    'min_samples_split': randint(2, 20),  # Minimum samples required to split a node\n",
    "    'min_samples_leaf': randint(1, 10),  # Minimum samples required in a leaf node\n",
    "    'max_features': [None, 'sqrt', 'log2']  # Number of features to consider for best split\n",
    "}\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=dt_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Evaluate based on AUC\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,  # Show progress\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Train the best model on the balanced training set\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate and display AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 3, 'n_estimators': 181}\n",
      "\n",
      "AUC Score: 0.6700\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3000 2218]\n",
      " [2082 4107]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.57      0.58      5218\n",
      "           1       0.65      0.66      0.66      6189\n",
      "\n",
      "    accuracy                           0.62     11407\n",
      "   macro avg       0.62      0.62      0.62     11407\n",
      "weighted avg       0.62      0.62      0.62     11407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 300),  # Randomly sample between 50 and 300 trees\n",
    "    'max_depth': [None, 10, 20, 30],  # Fixed choices for tree depth\n",
    "    'min_samples_split': randint(2, 11),  # Randomly sample between 2 and 10 for splitting\n",
    "    'min_samples_leaf': randint(1, 5),  # Randomly sample between 1 and 4 for leaf nodes\n",
    "    'max_features': ['sqrt', 'log2', None],  # Fixed options for max features\n",
    "    'bootstrap': [True, False]  # Fixed options for bootstrapping\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Evaluate based on AUC\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,  # Show progress\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Train the best model on the balanced training set\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate and display AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importances:\n",
      "\n",
      "                            Feature  Importance\n",
      "8  mean_migraine_duration_past7days    0.195429\n",
      "6        sleep_duration_past_7_days    0.187896\n",
      "1             attack_duration_hours    0.187863\n",
      "5              sleep_duration_hours    0.181959\n",
      "2                    pain_intensity    0.109953\n",
      "0           migraine_days_per_month    0.068042\n",
      "7        migraine_attacks_past7days    0.041139\n",
      "3                    trigger_stress    0.014386\n",
      "4                trigger_poor_sleep    0.013334\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "feature_importances = best_model.feature_importances_\n",
    "feature_names = X_train_balanced.columns\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\\n\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best Hyperparameters: {'learning_rate': 0.02906750508580709, 'max_depth': 9, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 6, 'n_estimators': 148, 'subsample': 0.8773893363123181}\n",
      "\n",
      "AUC Score: 0.6763\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3071 2147]\n",
      " [2110 4079]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.59      0.59      5218\n",
      "           1       0.66      0.66      0.66      6189\n",
      "\n",
      "    accuracy                           0.63     11407\n",
      "   macro avg       0.62      0.62      0.62     11407\n",
      "weighted avg       0.63      0.63      0.63     11407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 200),  # Number of boosting stages\n",
    "    'learning_rate': uniform(0.01, 0.3),  # Learning rate for boosting\n",
    "    'max_depth': randint(3, 10),  # Maximum depth of individual estimators\n",
    "    'min_samples_split': randint(2, 10),  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': randint(1, 5),  # Minimum number of samples required to be a leaf node\n",
    "    'subsample': uniform(0.7, 0.3),  # Fraction of samples used for fitting each base learner\n",
    "    'max_features': ['sqrt', 'log2', None]  # Number of features to consider at each split\n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gb_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Evaluate based on AUC\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,  # Show progress\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Train the best model on the balanced training set\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate and display AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importances:\n",
      "\n",
      "                            Feature  Importance\n",
      "2                    pain_intensity    0.341989\n",
      "8  mean_migraine_duration_past7days    0.143419\n",
      "1             attack_duration_hours    0.142839\n",
      "6        sleep_duration_past_7_days    0.118998\n",
      "5              sleep_duration_hours    0.105180\n",
      "0           migraine_days_per_month    0.048041\n",
      "3                    trigger_stress    0.044794\n",
      "7        migraine_attacks_past7days    0.031106\n",
      "4                trigger_poor_sleep    0.023635\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "feature_importances = best_model.feature_importances_\n",
    "feature_names = X_train_balanced.columns\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\\n\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best Hyperparameters: {'metric': 'manhattan', 'n_neighbors': 40, 'p': 2, 'weights': 'distance'}\n",
      "\n",
      "AUC Score: 0.6511\n",
      "Confusion Matrix:\n",
      "[[3259 1959]\n",
      " [2518 3671]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.62      0.59      5218\n",
      "           1       0.65      0.59      0.62      6189\n",
      "\n",
      "    accuracy                           0.61     11407\n",
      "   macro avg       0.61      0.61      0.61     11407\n",
      "weighted avg       0.61      0.61      0.61     11407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter distributions\n",
    "param_distributions = {\n",
    "    'n_neighbors': randint(1, 50),  # Number of neighbors to test\n",
    "    'weights': ['uniform', 'distance'],  # Weighting strategies\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],  # Distance metrics\n",
    "    'p': randint(1, 3)  # distance parameter \n",
    "}\n",
    "\n",
    "# Initialize the K-Nearest Neighbors model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=knn_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Evaluate based on AUC\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,  # Show progress\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV to the training data\n",
    "random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the best model and hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Train the best model on the balanced training set\n",
    "best_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "\n",
    "# Calculate and display AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# kernel dies when jupyter notebook tries import tensor flow so I tried this code in my local machine and commented it here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Define features and target\n",
    "features = ['migraine_days_per_month','attack_duration_hours','pain_intensity', 'trigger_stress', 'trigger_poor_sleep','sleep_duration_hours','sleep_duration_past_7_days','migraine_attacks_past7days','mean_migraine_duration_past7days']\n",
    "\n",
    "X = df[features]\n",
    "y = df['affected_activity_QoL']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE to the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define a function to create the model\n",
    "def create_model(learning_rate=0.001, neurons=[64, 32, 16]):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons[0], activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "    model.add(Dense(neurons[1], activation='relu'))\n",
    "    model.add(Dense(neurons[2], activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model for compatibility with RandomizedSearchCV\n",
    "keras_clf = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# Define hyperparameter distributions\n",
    "param_distributions = {\n",
    "    \"model__learning_rate\": [0.001, 0.01, 0.1],\n",
    "    \"model__neurons\": [[64, 32, 16], [128, 64, 32], [32, 16, 8]],\n",
    "    \"batch_size\": [16, 32, 64],\n",
    "    \"epochs\": [20, 50, 100]\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=keras_clf,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=10,  # Number of random combinations to try\n",
    "    scoring='roc_auc',  # Focus on auc for hyperparameter tuning\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "random_search.fit(X_train_scaled, y_train_smote)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]  # Probabilities for the positive class\n",
    "y_pred_classes = (y_pred_proba > 0.5).astype(int)  # Convert probabilities to binary classes\n",
    "\n",
    "# Calculate AUC\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Confusion Matrix and Classification Report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_classes))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "RESULTS:\n",
    "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
    "\n",
    "Best Hyperparameters: {'model__neurons': [64, 32, 16], 'model__learning_rate': 0.001, 'epochs': 20, 'batch_size': 16}\n",
    "\n",
    "AUC Score: 0.6680\n",
    "Confusion Matrix:\n",
    "[[3282 1936]\n",
    " [2443 3746]]\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.57      0.63      0.60      5218\n",
    "           1       0.66      0.61      0.63      6189\n",
    "\n",
    "    accuracy                           0.62     11407\n",
    "   macro avg       0.62      0.62      0.62     11407\n",
    "weighted avg       0.62      0.62      0.62     11407\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
